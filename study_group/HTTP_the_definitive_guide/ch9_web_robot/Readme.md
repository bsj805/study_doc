# 9 장 웹로봇

사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다.
- 콘텐츠를 가져오고
- 하이퍼 링크를 따라가고
- 데이터 처리

방식에 따라 분류
- 크롤러
- 스파이더
- 웜
- 봇


ex.)
- 주식시장 서버에 매분마다 HTTP GET
- www 웹 통계조사 로봇
- 검색DB위한 검색엔진 로봇
- 상품에 대한 가격 DB 만들기 위해 각 온라인 쇼핑몰 웹페이지 수집


# 9.1 크롤러, 크롤링

웹을 돌아다니며 모든 문서를 가져와 DB로 만든다.
- 크롤러 혹은 스파이더로 불림

## 9.1.1 어디서 시작할지: 루트 집합
- 모든 하이퍼링크 따라갔을때 관심 있는 페이지 전체 가져올 수 있게 해야 한다.
  - 최종적으로 모든 문서로 이어지는 하나의 루트문서는 없기 때문에 다양하게 시작해야 한다.
    - 크고 인기있는 웹사이트 (메인)
    - 새로 생성된 페이지들 목록 (갱신)
    - 자주 링크 되지 않는 페이지들의 목록 (보조)
      - 다음엔 안나오는데 네이버에 나온다면 크롤링 문서 집합의 차이 or 검색엔진 색인의 차이 등

## 9.1.2 링크 추출과 상대 링크 정상화
- 검색한 각 페이지 안의 URL 링크를 파싱 , 크롤링할 페이지들의 목록에 추가해야함.
  - BFS처럼 계속 추가됨.
  - 상대링크는 절대 링크로 바꿔야한다.
  - 단 순환 피해야 한다. 최소한 자기가 어디 방문했는지 정도는 알아야함.
  - 중복된 페이지 가져오는것도 피해야 한다.
    

그럼 어떻게 피하는가
- 방문한 곳을 지속적으로 추적하면 메모리가 터져버릴것.
- TRIE? 아무튼 해시테이블에 검색을 하던가 해서 내 방문 여부를 알아내야 할 것.
- 5억개 URL == 20GB 
  - 트리 & 해시 테이블
  - loose bitmap
    - 각 URL별 해시함수 적용해서 bitmap 칠하기 (presence bit)
  - 체크포인트
    - URL 목록이 디스크에 저장되었는지 다 확인하기
  - 파티셔닝
    - 한 대에서 하나의 로봇이 크롤링 완수하기 불가능.
    - 각 서버에서 하나씩 로봇이 일하는 팜(farm) 
      - 서로 커뮤니케이션 필요


Q. 한 URL이 다른 URL의 별칭이면 어떡할까?
- 포트가 다르거나
- encoding (한글주소 -> 인코딩된 주소)
- 서버가 대소문자 rsc 구분 안하고 리턴해줄때
- 아이피주소로 접근할때

A. 정규화를 한다.
- 포트번호 :80추가
- escape문자 원래대로 변환
- # 태그 제거

이걸로 안되는건 크롤링 된 데이터 확인하거나 해야함


## 9.1.8 파일 시스템 링크 순환
Q. FS의 파일끼리 순환링크면 어떡하나?
  - ex.) http://www.foo.com/subdir/subdir/subdir/index.html 이 subdir로 연결되어있다면
  - URL 한계 length있어서 다행 ^^
    - 달력 다음달버튼을 계속눌러 넘어가는 .. 동적컨텐츠 크롤링중이라면 어떡할것?
A. 휴리스틱하게 잘 풀어야한다.
  - 유효한 컨텐츠도 걍 잘라
  - URL 정규화
  - 너비 우선 크롤링 ( 순환중이더라도 다른 로봇들이 알아서 유효한 URL 가져올테니까 일정시간 후에 끝내게 하면 적당히 괜찮은 수준의 문서들 가져온다)
  - 스트롤링 : 페이지 숫자 제한하자.
  - URL 크기 제한
    - 로봇은 일정 크기 넘는 URL 크롤링 거부할 수 있다.
      - 서버는 DDOS인줄 알 수도 있다. 
  - URL/사이트 블랙리스트
    - 사람 손으로 추가해야지뭐..
  - 패턴 발견
    - 반복되는 url 일정이상 가지면 끝내
  - 콘텐츠 지문
    - checksum - 컨텐츠 똑같으면 크롤링 안해
    - MD5같은 메시지 요약 함수
    - 컨텐츠에 임베딩된 링크라면 조금씩 계속 바뀔 수 있으니 그것 외의 부분들만 checksum 하는 느낌으로 접근 가능
  - 사람의 모니터링
    - 특정 로봇 혼자 부하가 폭증하고있다면 알람 & 로깅 & 진단.


# 9.2 로봇의 HTTP
- 그냥 HTTP 클라이언트인데
  - User Agent로 로봇이라고 해서 보내야 잘못된 크롤러한테는 원서버가 정보 줄 수 있다. ( 내 서버에서 계속 트래픽을 먹고있어요 ㅠㅠ )
    - FROM : 로봇 관리자 의 이메일 주소
    - Accept: 어떤 미디어 타입 달라

## 9.2.2 가상 호스팅
- 하나의 서버에서 URL 두개 띄우는 가상 호스팅
  - 로봇은 자기가 어떤 HOST로 요청하는건지 헤더에 넣어야한다.
  - default 서버에서 받아온 걸 특정 URL에서 받아온거라고 생각할 수도 있다.

## 9.2.3 조건부 요청
로봇은 검색하는 컨텐츠 양을 최소화 할수록 좋다.
- 프록시 처럼 시간이나 엔티티 태그 비교해서 조건부 HTTP 요청

## 9.2.4 응답 다루기
- 보통은 그냥 저장하긴 하지만 HTTP 상태 코드는 이해해야한다.
- HTTP 헤더에 임베딩된 정보 따라서 엔티티 자체의 정보 찾기
  - 파싱하면 HTTP 헤더 되는 메타 HTML태그 같은거 

## 9.2.5 User-Agent 타기팅
- 사이트는 로봇에 많이 노출될수록 웬만하면 좋기 때문에, robot에게 컨텐츠 잘 주게 기능개발해야 한다. (js로만 나와서 중요한정보가 못나간다던가)

# 9.3 부적절하게 동작하는 로봇들

폭주하는 로봇
- 빠르게 HTTP -> DDOS
  - 적당히 빠르게 텀 두면서 해라

오래된 URL
- URL 존재하지 않는데 자꾸 보내서 에러로그가 폭증하거나 (ㅠㅠ) -> 그럼 다 살펴봐야함. (일반적)
  - 에러 페이지 제공하는 부하로 서버를 괴롭히는걸 싫어할 수도

길고 잘못된 URL
  - URL 충분히 길면 웹서버 처리 능력에 영향준다.

호기심 지나친 로봇
- 사적 데이터를 보게된경우
  - 민감한 데이터 검색엔진에서 제외시키거나 해야해.

동적 게이트웨이 접근
  - 게이트웨이 API로 URL 요청할수 있다.
    - 말도안되는 행위

# 9.4 로봇 차단하기
Robots.txt 표준 (Robots Exclusion Standard)
- robots.txt 먼저 요청해서 특정 문서 받아와도 되는지 파악한다.
- 두번의 HTTP 요청을 하는셈
- 임시방편이긴 하다. (실제 표준은 안만들어졌다고 볼 수 있다)
- 가상 호스팅된다면 각각의 Docroot에 별개의 robots.txt 있을 수 있다.
  - 없으면 서버 리소스 모두 허용하는셈

```html
User-Agent: 허용하는 agent이름
Disallow: /private

```
특정 User-Agent에 대한 Disallow나 allow인것.

/private 을 막았으면 /private123 도 막힌다. 

## 9.4.5 robots.txt의 캐싱과 만료
로봇은 주기적으로 roobts.txt를 가져와 캐싱해서 사용한다.

HTML에 로봇 제어 메타 태그를 넣어도 된다.
- 컨텐츠별로 컨텐츠 제작자가 로봇의 접근 제어를 할 수 있는 셈
- 무시해라
- 이페이지의 링크들은 크롤하지 말아라
- 인덱싱해라
- 크롤링해라
- 캐싱하지말아라

물론 로봇이 구현했어야 따라준다. 

# 9.5 로봇 에티켓
- 신원 식별
- 동작 (계속 로봇 동작 트래킹, 메일 수신 피드백 등)
- 스스로 제한 (robots.txt 따르기 등)
- 루프 중복 견디기 (URL 정규화 등 해라)
- 확장성 (로봇이 얼마나 많은 문서 커버할건지 계산을 해둬야한다. 그에맞춰 장비 설정하고)
- 신뢰성 (내부에 테스트 하고 내보내라)
- 소통 (robot 불만에 대응해줘)

# 9.6 검색 엔진
검색 엔진 동작 방법

1.full-text index 라는 로컬 DB 생성
  - 모든 문서에 대한 카탈로그로 동작
  - 검색하면 나오게 색인을 하는거지.
  - 색인 후에는 문서는 메모리에 읽을 일이없다. 왜냐하면 이제부턴 유저한테 링크만 제공해줄거니까

2. 질의 보내기
- 사용자가 HTML 폼으로 질의

3. 검색 결과 정렬 보여주기
- 결과 랭킹
- 랭킹로직은 아주 중요하다. 모델링 기법, 인기도 반영 가중치 등은 비밀스러워야함.
4. 스푸핑
- 검색결과 순서가 중요하다. 
- 이거 순위 높이려는 악성 유저도 있을것. -> 알고리즘 속이지 않게 계속 관련도 알고리즘 수정해야함.
